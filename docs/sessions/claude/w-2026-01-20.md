# Session: 2026-01-20 (Windows)

## Summary
- InvestigationDB: Rebuilt database from JSONL files
- /sf/data/ tree scan: Server-friendly commands with JSON output + mtime
- Parallel execution with xargs for multi-core machines

---

## InvestigationDB Rebuild

### Commands
```bash
cd C:\Users\thanu\Downloads\inverstigationdb

# Delete old DB and rebuild
rm db/investigation.db
python scripts/load_jsonl.py --reset
```

### Database Stats
| Entity | Records |
|--------|---------|
| Delta Tables | 3,702 |
| File Patterns | 4,947 |
| Source Patterns | 3,030 |
| Raw Enriched Paths | 1,451 |
| Source Paths | 149 |
| Dataset Repos | 128 |
| Vendors | 36 |

### Example Queries
```python
# Find delta table source
SELECT t.sf_table_name, p.directory_path
FROM delta_table t
JOIN table_path_mapping m ON t.table_id = m.table_id
JOIN raw_enriched_path p ON m.path_id = p.path_id
WHERE t.sf_table_name LIKE '%gics%';
```

---

## /sf/data/ Tree Scan Commands

### Basic Tree with JSON + mtime (Files & Folders)
```bash
nice -n 19 ionice -c2 -n7 tree -J -D --timefmt '%Y-%m-%d %H:%M:%S' /sf/data > ~/sf_data_tree.json
```

### Flags Explained
| Flag | Purpose |
|------|---------|
| `-J` | JSON output |
| `-D` | Include modification time |
| `--timefmt` | Date format |
| `-L 4` | Depth limit (optional) |
| `-d` | Directories only (optional) |
| `nice -n 19` | Lowest CPU priority |
| `ionice -c2 -n7` | Lowest I/O priority |

### Per-Vendor with Progress (Sequential - 1 Core)
```bash
for vendor in /sf/data/*/; do
  echo "[$(date '+%H:%M:%S')] Scanning: $(basename $vendor)"
  nice -n 19 ionice -c2 -n7 tree -J -D --timefmt '%Y-%m-%d %H:%M:%S' "$vendor" > ~/tree_$(basename $vendor).json
done && echo "[$(date '+%H:%M:%S')] Done!"
```

### Parallel Execution (Multi-Core)
```bash
# Use 2 cores (safe for 4-core machine)
ls -d /sf/data/*/ | xargs -P 2 -I {} bash -c '
  vendor=$(basename "{}")
  echo "[$(date "+%H:%M:%S")] Scanning: $vendor"
  nice -n 19 ionice -c2 -n7 tree -J -D --timefmt "%Y-%m-%d %H:%M:%S" "{}" > ~/tree_${vendor}.json
'
echo "[$(date '+%H:%M:%S')] Done!"
```

### Parallel Options
| Option | Cores | Speed | Server Impact |
|--------|-------|-------|---------------|
| Sequential | 1 | Slow | Minimal |
| `-P 2` | 2 | 2x faster | Low |
| `-P 4` | 4 | 4x faster | Medium |

### Check Machine Cores
```bash
nproc
```

### Alternative: Find with Real-time Progress (JSONL)
```bash
nice -n 19 ionice -c2 -n7 find /sf/data -printf '{"path":"%p","type":"%y","mtime":"%TY-%Tm-%Td %TH:%TM:%TS","size":%s}\n' 2>/dev/null > ~/sf_data_tree.jsonl &

# Watch progress
watch -n 5 'wc -l ~/sf_data_tree.jsonl'
```

### Background Execution with nohup
```bash
nohup nice -n 19 ionice -c2 -n7 tree -J -D --timefmt '%Y-%m-%d %H:%M:%S' /sf/data > ~/sf_data_tree.json 2>&1 &

# Check if running
ps aux | grep tree

# Check file size
ls -lh ~/sf_data_tree.json
```

---

## JSON Output Format (tree -J)

```json
[
  {"type":"directory","name":"bloomberg","time":"2026-01-19 09:00:00","contents":[
    {"type":"directory","name":"best_cwiq_pipe","time":"2026-01-19 08:00:00","contents":[
      {"type":"directory","name":"1.0","time":"2026-01-19 08:00:00","contents":[
        {"type":"directory","name":"raw","time":"2026-01-19 07:30:00","contents":[
          {"type":"file","name":"bestAmer.csv.20260119","time":"2026-01-19 07:30:00"}
        ]}
      ]}
    ]}
  ]}
]
```

---

## Process Management

### List Your Processes
```bash
ps aux | grep $USER
ps -u $USER -o pid,cmd,%cpu,%mem,etime
```

### Kill Background Jobs
```bash
# Kill background jobs only
kill $(jobs -p) 2>/dev/null

# Kill specific PID
kill <PID>

# Find tree/find processes
ps -u $USER -o pid,cmd | grep -E 'tree|find'
```

---

## Notes
- `tree -J` buffers entire output, writes at end (no real-time progress with `tail -f`)
- `pv` not installed on prod server
- Use per-vendor loop for visible progress
- Use `xargs -P 2` for parallel on 4-core machine (leaves 2 cores for others)
- Server: ny5-predpalch02

---

## Next Steps
- [ ] Run tree scan on prod server
- [ ] Copy JSON files to local
- [ ] Create parser script for InvestigationDB
- [ ] Create `scripts/_shared/` library
- [ ] Update `load_jsonl.py` for v2 tables

---

## InvestigationDB Schema v2 Refactor

### Goal
Enable end-to-end lineage in ONE query:
```
delta_table → raw_enriched → beacon_raw → gold → silver → bronze → source
```

### Before vs After

| Before | After |
|--------|-------|
| 12+ tables split by layer | Unified tables with `layer` column |
| 6+ joins for lineage | Single recursive query |
| No owner tracking on beacon_raw | `beacon_raw_path.owner_id` |

### New Schema Tables

**Dimension Tables:**
- `vendor` - Data vendors (sp, bloomberg, factset)
- `owner` - Path owners (cwiq-pipe, data-alchemy, cds-jobs, cdp-sync, sf-storage-migration)
- `dataset_repo` - SF dataset repositories

**Raw Enriched (3 tables):**
- `raw_enriched_path` - Directory paths
- `raw_enriched_file_pattern` - File patterns (pattern, regex, example)
- `raw_enriched` - Combination: path_id + pattern_id + full_path_example

**Beacon Raw (4 tables - normalized):**
- `beacon_raw_path` - Base directory paths with `owner_id`
- `beacon_raw_dir_pattern` - Nested directory structure (e.g., `YYYY/YYYYMMDD/`)
- `beacon_raw_file_pattern` - Filename patterns (human readable, regex, example)
- `beacon_raw` - Combination: path_id + dir_pattern_id + file_pattern_id

**Pipeline Layers (unified):**
- `path` - All layer paths (source, bronze, silver, gold) with layer column
- `pattern` - All layer patterns with layer column
- `layer_mapping` - source→bronze→silver→gold mappings

**Fact/Mapping Tables:**
- `delta_table` - SF tables with `raw_enriched_id` FK (only sf_schema + sf_table_name)
- `raw_enriched.beacon_raw_id` - Direct FK to beacon_raw (no mapping table needed)
- `gold_to_beacon_raw` - Links gold → beacon_raw

**Utility:**
- `filesystem_snapshot` - Tree scan results for pattern matching

### Key Design Decisions

1. **delta_table** - Only `sf_schema` + `sf_table_name` (no vendor_id - derived through lineage)
2. **beacon_raw_path** - Has `owner_id` for multiple owners:
   - `data-alchemy` (id=2)
   - `cdp-sync` / shovel (id=4)
   - `sf-storage-migration` (id=5)
3. **beacon_raw normalized** - Separated into path + dir_pattern + file_pattern for flexibility
4. **raw_enriched.beacon_raw_id** - Direct FK instead of separate mapping table (cds-jobs knows which beacon_raw it reads)
5. **File pattern format** - Human readable (`bestAmer|....csv`), regex, and example

### Beacon Raw Normalization

```
beacon_raw_path           -- /sf/data/vendor/dataset/1.0/raw/
     │
beacon_raw_dir_pattern    -- YYYY/YYYYMMDD/
     │
beacon_raw_file_pattern   -- bestAmer|....csv (human), regex, example
     │
     └──► beacon_raw (combination)
```

### Files Modified

```
inverstigationdb/
├── schema/schema.sql          # v2 unified schema
├── CLAUDE.md                  # Updated schema diagram
├── README.md                  # Full rewrite with Mermaid ERD
└── data/
    ├── owner.jsonl            # Populated (5 owners)
    ├── path.jsonl             # Empty (unified)
    ├── pattern.jsonl          # Empty (unified)
    ├── layer_mapping.jsonl    # Empty
    ├── beacon_raw_path.jsonl  # Empty
    ├── beacon_raw_dir_pattern.jsonl  # NEW
    ├── beacon_raw_file_pattern.jsonl # NEW (renamed)
    ├── beacon_raw.jsonl
    ├── raw_enriched.jsonl
    ├── raw_enriched_file_pattern.jsonl
    ├── gold_to_beacon_raw.jsonl
    └── filesystem_snapshot.jsonl
```

### PowerShell SCP Shortcut
```powershell
# xcp function in profile for alc1-6 servers
xcp alc1 -src /remote/path -des ./local/
xcp alc1 -up -src ./local/ -des /remote/path/
```

---

## ACL Checker Refactor

### Branch & MR
- Branch: `feature/acl-checker-refactor`
- MR: https://git.codewilling.com/data/cwiq-pipe/data-alchemy/-/merge_requests/465

### Code Reduction
| Change | Lines |
|--------|-------|
| Initial | 607 |
| Merged `format_env_range` → `format_env_pattern` | -22 |
| Simplified `print_check_result` | -4 |
| **Final** | **581** |

### Key Changes

**1. Layer ACL Config**
```python
@attrs.define(frozen=True, slots=True)
class LayerACL:
    need_default: bool = False

LAYER_CONFIG = {
    "raw": LayerACL(need_default=True),
    "env": LayerACL(need_default=True),
    "bronze": LayerACL(),
    ...
}
```

**2. Unified `get_acl()` function**
- Single function for recursive and non-recursive
- Uses `getfacl -e [-R]`
- Supports configurable `expected` permission
- Auto-detects owner access

**3. `format_env_pattern()` - glob-style grouping**
```python
# Input: ok_names=["env0","env2","env4"], total=6
# Output: "env[0,2,4]" (inclusion)

# Input: ok_names=["env0","env2","env3","env4","env5"], total=6
# Output: "env[!1]" (exclusion)

# Input: all OK → "env*"
```

**4. Regex patterns (support dots in usernames)**
```python
RE_ACL_OWNER = re.compile(r"# owner: ([\w.]+)")
RE_ACL_USER = re.compile(r"^user:([\w.]*):([rwx-]+)", re.MULTILINE)
RE_ACL_DEFAULT = re.compile(r"^default:user:([\w.]*):([rwx-]+)", re.MULTILINE)
```

### CLI Flags

| Flag | Short | Default | Description |
|------|-------|---------|-------------|
| `--vendor` | `-v` | None | Vendor name |
| `--dataset` | `-d` | None | Dataset name |
| `--version` | `-V` | `1.0` | Version |
| `--dataset-path` | `-p` | None | Full dataset path |
| `--check` | `-c` | None | Check any path |
| `--user` | `-u` | `svc_dat_alchemy` | User to verify |
| `--expected` | | `rwx` | Expected permission |
| `--issues-only` | `-i` | False | Grouped list output |
| `--fp-prefix` | | `/sf/data` | Data root |
| `--env` | `-e` | `env*` | Env pattern filter |
| `--recursive` | `-R` | False | Recursive scan |

### Usage Examples
```bash
# Dataset mode
acl_checker.py -v bloomberg -d best_cwiq_pipe

# Check arbitrary path (recursive)
acl_checker.py -c /sf/data/vendor/dataset/1.0/raw -R

# Issues only, grouped output
acl_checker.py -c /sf/data/vendor/dataset/1.0/raw -R -i

# Check for read-only permission
acl_checker.py -c /some/path --expected r-x
```

### Output with `-i` (issues-only)
```
Missing Regular:
  /sf/data/.../env0
  /sf/data/.../env1/file.csv

Missing Default:
  /sf/data/.../env2

Total: 3
```

### Pending
- [ ] Auto-recursive for directories (remove `-R` flag for `--check` mode)

### Test Coverage
- 35 tests passing
- Tests in `tests/acl_checker_tests.py`
