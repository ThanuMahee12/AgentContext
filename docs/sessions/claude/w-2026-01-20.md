# Session: 2026-01-20 (Windows)

## Summary
- InvestigationDB: Rebuilt database from JSONL files
- /sf/data/ tree scan: Server-friendly commands with JSON output + mtime
- Parallel execution with xargs for multi-core machines
- Git submodule update: data-alchemy updated (reuters_fundamentals, bbocax futures)
- Windows storage cleanup: pip cache (215MB), npm cache, temp files, .claude transcripts

---

## InvestigationDB Rebuild

### Commands
```bash
cd C:\Users\thanu\Downloads\inverstigationdb

# Delete old DB and rebuild
rm db/investigation.db
python scripts/load_jsonl.py --reset
```

### Database Stats
| Entity | Records |
|--------|---------|
| Delta Tables | 3,702 |
| File Patterns | 4,947 |
| Source Patterns | 3,030 |
| Raw Enriched Paths | 1,451 |
| Source Paths | 149 |
| Dataset Repos | 128 |
| Vendors | 36 |

### Example Queries
```python
# Find delta table source
SELECT t.sf_table_name, p.directory_path
FROM delta_table t
JOIN table_path_mapping m ON t.table_id = m.table_id
JOIN raw_enriched_path p ON m.path_id = p.path_id
WHERE t.sf_table_name LIKE '%gics%';
```

---

## /sf/data/ Tree Scan Commands

### Basic Tree with JSON + mtime (Files & Folders)
```bash
nice -n 19 ionice -c2 -n7 tree -J -D --timefmt '%Y-%m-%d %H:%M:%S' /sf/data > ~/sf_data_tree.json
```

### Flags Explained
| Flag | Purpose |
|------|---------|
| `-J` | JSON output |
| `-D` | Include modification time |
| `--timefmt` | Date format |
| `-L 4` | Depth limit (optional) |
| `-d` | Directories only (optional) |
| `nice -n 19` | Lowest CPU priority |
| `ionice -c2 -n7` | Lowest I/O priority |

### Per-Vendor with Progress (Sequential - 1 Core)
```bash
for vendor in /sf/data/*/; do
  echo "[$(date '+%H:%M:%S')] Scanning: $(basename $vendor)"
  nice -n 19 ionice -c2 -n7 tree -J -D --timefmt '%Y-%m-%d %H:%M:%S' "$vendor" > ~/tree_$(basename $vendor).json
done && echo "[$(date '+%H:%M:%S')] Done!"
```

### Parallel Execution (Multi-Core)
```bash
# Use 2 cores (safe for 4-core machine)
ls -d /sf/data/*/ | xargs -P 2 -I {} bash -c '
  vendor=$(basename "{}")
  echo "[$(date "+%H:%M:%S")] Scanning: $vendor"
  nice -n 19 ionice -c2 -n7 tree -J -D --timefmt "%Y-%m-%d %H:%M:%S" "{}" > ~/tree_${vendor}.json
'
echo "[$(date '+%H:%M:%S')] Done!"
```

### Parallel Options
| Option | Cores | Speed | Server Impact |
|--------|-------|-------|---------------|
| Sequential | 1 | Slow | Minimal |
| `-P 2` | 2 | 2x faster | Low |
| `-P 4` | 4 | 4x faster | Medium |

### Check Machine Cores
```bash
nproc
```

### Alternative: Find with Real-time Progress (JSONL)
```bash
nice -n 19 ionice -c2 -n7 find /sf/data -printf '{"path":"%p","type":"%y","mtime":"%TY-%Tm-%Td %TH:%TM:%TS","size":%s}\n' 2>/dev/null > ~/sf_data_tree.jsonl &

# Watch progress
watch -n 5 'wc -l ~/sf_data_tree.jsonl'
```

### Background Execution with nohup
```bash
nohup nice -n 19 ionice -c2 -n7 tree -J -D --timefmt '%Y-%m-%d %H:%M:%S' /sf/data > ~/sf_data_tree.json 2>&1 &

# Check if running
ps aux | grep tree

# Check file size
ls -lh ~/sf_data_tree.json
```

---

## JSON Output Format (tree -J)

```json
[
  {"type":"directory","name":"bloomberg","time":"2026-01-19 09:00:00","contents":[
    {"type":"directory","name":"best_cwiq_pipe","time":"2026-01-19 08:00:00","contents":[
      {"type":"directory","name":"1.0","time":"2026-01-19 08:00:00","contents":[
        {"type":"directory","name":"raw","time":"2026-01-19 07:30:00","contents":[
          {"type":"file","name":"bestAmer.csv.20260119","time":"2026-01-19 07:30:00"}
        ]}
      ]}
    ]}
  ]}
]
```

---

## Process Management

### List Your Processes
```bash
ps aux | grep $USER
ps -u $USER -o pid,cmd,%cpu,%mem,etime
```

### Kill Background Jobs
```bash
# Kill background jobs only
kill $(jobs -p) 2>/dev/null

# Kill specific PID
kill <PID>

# Find tree/find processes
ps -u $USER -o pid,cmd | grep -E 'tree|find'
```

---

## Notes
- `tree -J` buffers entire output, writes at end (no real-time progress with `tail -f`)
- `pv` not installed on prod server
- Use per-vendor loop for visible progress
- Use `xargs -P 2` for parallel on 4-core machine (leaves 2 cores for others)
- Server: ny5-predpalch02

---

## Next Steps
- [ ] Run tree scan on prod server
- [ ] Copy JSON files to local
- [ ] Create parser script for InvestigationDB
- [ ] Create `scripts/_shared/` library
- [ ] Update `load_jsonl.py` for v2 tables

---

## InvestigationDB Schema v2 Refactor

### Goal
Enable end-to-end lineage in ONE query:
```
delta_table → raw_enriched → beacon_raw → gold → silver → bronze → source
```

### Before vs After

| Before | After |
|--------|-------|
| 12+ tables split by layer | Unified tables with `layer` column |
| 6+ joins for lineage | Single recursive query |
| No owner tracking on beacon_raw | `beacon_raw_path.owner_id` |

### New Schema Tables

**Dimension Tables:**
- `vendor` - Data vendors (sp, bloomberg, factset)
- `owner` - Path owners (cwiq-pipe, data-alchemy, cds-jobs, cdp-sync, sf-storage-migration)
- `dataset_repo` - SF dataset repositories

**Raw Enriched (3 tables):**
- `raw_enriched_path` - Directory paths
- `raw_enriched_file_pattern` - File patterns (pattern, regex, example)
- `raw_enriched` - Combination: path_id + pattern_id + full_path_example

**Beacon Raw (4 tables - normalized):**
- `beacon_raw_path` - Base directory paths with `owner_id`
- `beacon_raw_dir_pattern` - Nested directory structure (e.g., `YYYY/YYYYMMDD/`)
- `beacon_raw_file_pattern` - Filename patterns (human readable, regex, example)
- `beacon_raw` - Combination: path_id + dir_pattern_id + file_pattern_id

**Pipeline Layers (unified):**
- `path` - All layer paths (source, bronze, silver, gold) with layer column
- `pattern` - All layer patterns with layer column
- `layer_mapping` - source→bronze→silver→gold mappings

**Fact/Mapping Tables:**
- `delta_table` - SF tables with `raw_enriched_id` FK (only sf_schema + sf_table_name)
- `raw_enriched.beacon_raw_id` - Direct FK to beacon_raw (no mapping table needed)
- `gold_to_beacon_raw` - Links gold → beacon_raw

**Utility:**
- `filesystem_snapshot` - Tree scan results for pattern matching

### Key Design Decisions

1. **delta_table** - Only `sf_schema` + `sf_table_name` (no vendor_id - derived through lineage)
2. **beacon_raw_path** - Has `owner_id` for multiple owners:
   - `data-alchemy` (id=2)
   - `cdp-sync` / shovel (id=4)
   - `sf-storage-migration` (id=5)
3. **beacon_raw normalized** - Separated into path + dir_pattern + file_pattern for flexibility
4. **raw_enriched.beacon_raw_id** - Direct FK instead of separate mapping table (cds-jobs knows which beacon_raw it reads)
5. **File pattern format** - Human readable (`bestAmer|....csv`), regex, and example

### Beacon Raw Normalization

```
beacon_raw_path           -- /sf/data/vendor/dataset/1.0/raw/
     │
beacon_raw_dir_pattern    -- YYYY/YYYYMMDD/
     │
beacon_raw_file_pattern   -- bestAmer|....csv (human), regex, example
     │
     └──► beacon_raw (combination)
```

### Files Modified

```
inverstigationdb/
├── schema/schema.sql          # v2 unified schema
├── CLAUDE.md                  # Updated schema diagram
├── README.md                  # Full rewrite with Mermaid ERD
└── data/
    ├── owner.jsonl            # Populated (5 owners)
    ├── path.jsonl             # Empty (unified)
    ├── pattern.jsonl          # Empty (unified)
    ├── layer_mapping.jsonl    # Empty
    ├── beacon_raw_path.jsonl  # Empty
    ├── beacon_raw_dir_pattern.jsonl  # NEW
    ├── beacon_raw_file_pattern.jsonl # NEW (renamed)
    ├── beacon_raw.jsonl
    ├── raw_enriched.jsonl
    ├── raw_enriched_file_pattern.jsonl
    ├── gold_to_beacon_raw.jsonl
    └── filesystem_snapshot.jsonl
```

### PowerShell SCP Shortcut
```powershell
# xcp function in profile for alc1-6 servers
xcp alc1 -src /remote/path -des ./local/
xcp alc1 -up -src ./local/ -des /remote/path/
```

---

## ACL Checker Refactor

### Branch & MR
- Branch: `feature/acl-checker-refactor`
- MR: https://git.codewilling.com/data/cwiq-pipe/data-alchemy/-/merge_requests/465

### Code Reduction
| Change | Lines |
|--------|-------|
| Initial | 607 |
| Merged `format_env_range` → `format_env_pattern` | -22 |
| Simplified `print_check_result` | -4 |
| **Final** | **581** |

### Key Changes

**1. Layer ACL Config**
```python
@attrs.define(frozen=True, slots=True)
class LayerACL:
    need_default: bool = False

LAYER_CONFIG = {
    "raw": LayerACL(need_default=True),
    "env": LayerACL(need_default=True),
    "bronze": LayerACL(),
    ...
}
```

**2. Unified `get_acl()` function**
- Single function for recursive and non-recursive
- Uses `getfacl -e [-R]`
- Supports configurable `expected` permission
- Auto-detects owner access

**3. `format_env_pattern()` - glob-style grouping**
```python
# Input: ok_names=["env0","env2","env4"], total=6
# Output: "env[0,2,4]" (inclusion)

# Input: ok_names=["env0","env2","env3","env4","env5"], total=6
# Output: "env[!1]" (exclusion)

# Input: all OK → "env*"
```

**4. Regex patterns (support dots in usernames)**
```python
RE_ACL_OWNER = re.compile(r"# owner: ([\w.]+)")
RE_ACL_USER = re.compile(r"^user:([\w.]*):([rwx-]+)", re.MULTILINE)
RE_ACL_DEFAULT = re.compile(r"^default:user:([\w.]*):([rwx-]+)", re.MULTILINE)
```

### CLI Flags

| Flag | Short | Default | Description |
|------|-------|---------|-------------|
| `--vendor` | `-v` | None | Vendor name |
| `--dataset` | `-d` | None | Dataset name |
| `--version` | `-V` | `1.0` | Version |
| `--dataset-path` | `-p` | None | Full dataset path |
| `--check` | `-c` | None | Check any path |
| `--user` | `-u` | `svc_dat_alchemy` | User to verify |
| `--expected` | | `rwx` | Expected permission |
| `--issues-only` | `-i` | False | Grouped list output |
| `--fp-prefix` | | `/sf/data` | Data root |
| `--env` | `-e` | `env*` | Env pattern filter |
| `--recursive` | `-R` | False | Recursive scan |

### Usage Examples
```bash
# Dataset mode
acl_checker.py -v bloomberg -d best_cwiq_pipe

# Check arbitrary path (recursive)
acl_checker.py -c /sf/data/vendor/dataset/1.0/raw -R

# Issues only, grouped output
acl_checker.py -c /sf/data/vendor/dataset/1.0/raw -R -i

# Check for read-only permission
acl_checker.py -c /some/path --expected r-x
```

### Output with `-i` (issues-only)
```
Missing Regular:
  /sf/data/.../env0
  /sf/data/.../env1/file.csv

Missing Default:
  /sf/data/.../env2

Total: 3
```

### Pending
- [ ] Auto-recursive for directories (remove `-R` flag for `--check` mode)

### Test Coverage
- 35 tests passing
- Tests in `tests/acl_checker_tests.py`

---

## Tree JSON Files Download

### Downloaded Files
Tree scan JSON files from alchmy_ny5p01 to local:
```
C:\Users\thanu\Downloads\inverstigationdb\context\
├── tree_bloomberg.json (528MB)
├── tree_blue_matrix.json (2.5GB)
├── tree_consumer_edge.json (2.1GB)
├── tree_cw.json (4.4GB)
└── ... (many more vendors)
```

### Zip & Transfer Commands

```bash
# OS-friendly zip on 4-core machine
ssh alchmy_ny5p01 "nice -n 19 zip ~/tree_all.zip ~/tree*.json && rm -f ~/tree*.json"

# Download
xcp alc1 -src ~/tree_all.zip -des "C:\Users\thanu\Downloads\inverstigationdb\context\"
```

### Monitor SCP Transfer

```bash
# On remote - see active SCP processes
ps aux | grep scp

# Watch file being written
watch -n 1 'ls -lh ~/tree_all.zip'

# Via SSH from PowerShell
ssh alchmy_ny5p01 "watch -n 1 'ls -lh ~/tree_all.zip'"
```

### Disk Usage Commands

```bash
# Machine full space
df -h

# Your home directory usage
du -sh ~

# Breakdown by folder
du -sh ~/* 2>/dev/null | sort -h
```

### Useful Find Commands

```bash
# Delete all files in output directory
find output -type f -delete

# Or with exec
find output -type f -exec rm -f {} +
```

---

## Quick Reference

### PowerShell History Search
```powershell
Get-History | Where-Object {$_.CommandLine -like "*scp*" -and $_.CommandLine -like "*keyword*"}
Get-Content (Get-PSReadlineOption).HistorySavePath | Select-String "pattern"
```

### xcp Usage (from PS profile)
```powershell
xcp alc1 -src /remote/path -des ./local/      # download
xcp alc1 -up -src ./local/ -des /remote/      # upload
xcp alc1 -r -v -src /remote/dir -des ./local/ # recursive + verbose
```

---

## ClickUp CLI Integration

### Setup
```bash
npm install -g clickup-cli
```

### Config File (~/.clickup.json)
```json
{
  "token": "pk_XXXXX_YOUR_TOKEN",
  "lists": {
    "default": "901807382430",
    "project1": "901807382431"
  }
}
```

### API Commands

```bash
# Get teams
curl -H "Authorization: pk_TOKEN" https://api.clickup.com/api/v2/team

# Get spaces
curl -H "Authorization: pk_TOKEN" "https://api.clickup.com/api/v2/team/TEAM_ID/space"

# Get folders/lists
curl -H "Authorization: pk_TOKEN" "https://api.clickup.com/api/v2/space/SPACE_ID/folder"

# Get task
curl -H "Authorization: pk_TOKEN" "https://api.clickup.com/api/v2/task/TASK_ID"

# Update task description
curl -X PUT \
  -H "Authorization: pk_TOKEN" \
  -H "Content-Type: application/json" \
  -d @desc.json \
  "https://api.clickup.com/api/v2/task/TASK_ID"

# Add comment
curl -X POST \
  -H "Authorization: pk_TOKEN" \
  -H "Content-Type: application/json" \
  -d '{"comment_text": "Your comment"}' \
  "https://api.clickup.com/api/v2/task/TASK_ID/comment"
```

### Workspaces
| Team | ID |
|------|-----|
| Test | 90181127218 |
| CodeWilling | 9005044077 |

### Updated Tasks
- **868gxrd8g** - ACL Checker Refactor (description updated with MR, code changes, CLI flags)

---

## Data-Alchemy: Backfill Date Range Feature (Planned)

### Ticket Description
**Add backfill sub-options**

Usage:
```bash
--backfill bk-hours 720
--backfill bk-range 20251201:20251215
```

Single `--backfill` flag with type prefix:
- `bk-hours N` - hours from now
- `bk-range YYYYMMDD:YYYYMMDD` - date range

### Current Implementation
- `--backfill N` in `main.py` line 236
- `discover_layer_files_from_disk()` filters by hours

### Files to Modify
- `data_alchemy/main.py`

---

## Git Submodule Updates

### Pull Command
```bash
git pull
git submodule update --recursive --remote
```

### data-alchemy Recent Commits
```
be9472c fix: use UTC date from silver folder for reuters_fundamentals timestamp suffix
710ab8b feat: add gzip compression and array validation for bbocax futures grabber map
50a4a4e revert: undo bbocax-futures changes (keep prod06 yaml)
fc52101 feat: add 3-hour backfill for bbocax and comment out trth_api in prod06
d4d46d6 fix: update futures grabber map validation to array format
```

---

## Windows Storage Cleanup

### Disk Status
- C: drive ~18GB free (~7.6% of 237GB)

### Folder Sizes (before cleanup)
| Folder | Size |
|--------|------|
| Downloads | 5.4 GB |
| pip cache | 210 MB |
| Temp | 190 MB |
| .claude | 186 MB |
| npm-cache | 137 MB |

### Cleanup Commands

**1. Check disk space:**
```powershell
Get-PSDrive C | Select-Object Used, Free
```

**2. Open Windows Disk Cleanup:**
```cmd
cleanmgr
```

**3. Clear Windows Temp files:**
```powershell
Remove-Item -Path "$env:TEMP\*" -Recurse -Force -ErrorAction SilentlyContinue
```

**4. Clear npm cache:**
```cmd
npm cache clean --force
```

**5. Clear pip cache (freed 215.6 MB):**
```cmd
pip cache purge
```

**6. Clear Docker unused data:**
```cmd
docker system prune -a
```

**7. Check large files in Downloads:**
```powershell
Get-ChildItem "$env:USERPROFILE\Downloads" | Sort-Object Length -Descending | Select-Object Name, @{N='SizeMB';E={[math]::Round($_.Length/1MB,2)}} -First 20
```

**8. Clear Claude Code transcripts:**
```powershell
Remove-Item "$env:USERPROFILE\.claude\projects\*\*.jsonl" -Force -ErrorAction SilentlyContinue
```

### Additional Cleanup Commands

**Check AppData\Local size:**
```powershell
Get-ChildItem "$env:LOCALAPPDATA" -Directory | ForEach-Object { $s=(Get-ChildItem $_.FullName -Recurse -Force -EA SilentlyContinue | Measure-Object Length -Sum).Sum; if($s -gt 500MB){[PSCustomObject]@{GB=[math]::Round($s/1GB,2);Name=$_.Name}}} | Sort-Object GB -Descending
```

**Windows Update cleanup (Admin):**
```cmd
Dism.exe /online /Cleanup-Image /StartComponentCleanup
```

**Find large node_modules:**
```powershell
Get-ChildItem -Path "$env:USERPROFILE" -Filter "node_modules" -Recurse -Directory -EA SilentlyContinue | ForEach-Object { $s=(Get-ChildItem $_.FullName -Recurse -Force -EA SilentlyContinue | Measure-Object Length -Sum).Sum; [PSCustomObject]@{GB=[math]::Round($s/1GB,2);Path=$_.FullName}} | Sort-Object GB -Descending
```
