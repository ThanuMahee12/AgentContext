# Session: 2026-01-27 (Windows)

## Context
- **Repo**: alchmy
- **Branch**: main, feature/backfill-date-range-clean, feature/bbocax-preferred-exch

## Goals
- [x] rsync command for bloomberg data copy
- [x] data-alchemy CLI options investigation
- [x] Review feature/backfill-date-range-clean branch
- [x] Review feature/bbocax-preferred-exch branch
- [x] Setup Python venv on remote server
- [ ] Per-dataset backfill hours configuration (TODO)

## Work Log

### rsync Command for Bloomberg Data

**Task**: Copy bronze files for dates Jan 19-26

#### Final Commands

```bash
# Full path version (dates 24-26)
mkdir -p /sf/proj/data_alchemy/thanudev/data/bloomberg/1.0/bronze/2026/01

seq 24 26 | xargs -P4 -I{} rsync -av --info=progress2 --no-compress --inplace --partial --ignore-existing /sf/data/bloomberg/bbocax_cwiq_pipe/1.0/bronze/2026/01/{} /sf/proj/data_alchemy/thanudev/data/bloomberg/1.0/bronze/2026/01/
```

```bash
# Variable version (dates 19-26)
SRC=/sf/data/bloomberg/bbocax_cwiq_pipe/1.0/bronze/2026/01
DST=/sf/home/bthanujan.mahendran/data/bloomberg/bbocax_cwiq_pipe/1.0/bronze/2026/01

mkdir -p "$DST"

seq 19 26 | xargs -P8 -I{} rsync -av --info=progress2 --no-compress --inplace --partial --ignore-existing "$SRC"/{} "$DST"/
```

**Options**:
| Option | Purpose |
|--------|---------|
| `--no-compress` | Skip compression (large files) |
| `--inplace` | Write directly (saves space) |
| `--partial` | Resume interrupted |
| `--ignore-existing` | Skip existing files |

---

### feature/backfill-date-range-clean Review

**New feature**: `--backfill-range YYYYMMDD:YYYYMMDD`

**Verdict**: ✅ Ready for merge with minor fixes optional
- Uses mtime for filtering
- Mutually exclusive with `--backfill` hours

---

### feature/bbocax-preferred-exch Review

**Commits**:
```
1fbe218 fix: correct regex pattern order for pfd_exch _cins_bbid suffix
e75e78d feat: add back_office_preferred_exch mapping for bbocax_cwiq_pipe
```

**Grabber Maps Added**:
- `bloomberg_bbocax_cwiq_pipe_preferred_exch_1_0.json` - CSV/dif/out/px files
- `bloomberg_bbocax_cwiq_pipe_preferred_exch_parquet_1_0.json` - parquet files

**Files Covered**:
| File Type | Pattern | Status |
|-----------|---------|--------|
| `pfdExchAsia.csv` | `pfdExch(?:Asia\|Euro\|...)(?:Cins)?\.csv` | ✅ |
| `pfdExchAsiaCins.csv` | Same pattern | ✅ |
| `pfdExchAsiaPricing.csv` | `pfdExch...Pricing\.csv` | ✅ |
| `pfdExchAsia.parquet` | Parquet map | ✅ |
| `pfdExchAsiaHistoricalPricing.parquet` | Parquet map | ✅ |
| `pfd_exch_asia.dif/out` | `pfd_exch_...\.(?:dif\|out)` | ✅ |
| `pfd_exch_asia.px` | `pfd_exch_...\.px` | ✅ |
| `pfd_exch_asia.px.hpc` | `pfd_exch_...\.px\.hpc` | ✅ |

**No Overlap with corporate_actions**:
- `preferred_exch` → NO `CorporateActions`, NO `.cax`
- `corporate_actions` → HAS `CorporateActions` OR `.cax`

**Verdict**: ✅ Branch correct, no overlap

---

### Python venv Setup on Remote Server

**Issue**: Python 3.9 on server, data-alchemy requires 3.13+

**Workaround**:
```bash
/opt/mpenv/bin/python -m venv .venv
.venv/bin/python -m ensurepip --upgrade
.venv/bin/python -m pip install "typer>=0.16.0,<0.17.0" "loguru>=0.7.3,<0.8.0" sqlmodel pytest "python-dotenv>=1.0.0" "PyYAML>=6.0" "sysrsync>=1.1.1" "tqdm>=4.66.0" "lz4>=4.0.0" "rich>=13.0.0" "pandas>=2.0.0" "pyarrow>=18.0.0,<22.0.0" "ujson>=5.11.0" "polars>=1.35.2" "attrs>=24.0.0" "filetype>=1.2.0"
```

**Note**: Use `.venv/bin/python -m pip` to avoid conda pip conflict.

---

### BBOCAX Split Service Commands

```bash
# 1. corporate_actions
python -m data_alchemy.main --vendor bloomberg --dataset bbocax_cwiq_pipe --version 1.0 --backfill 200 --ignore-db --file-pattern "(equity_(asia1|asia2|euro|lamr|namr)\.cax|(equity|effDt).+CorporateActions)"

# 2. back_office_futures
python -m data_alchemy.main --vendor bloomberg --dataset bbocax_cwiq_pipe --version 1.0 --backfill 200 --ignore-db --file-pattern "(share|nonShare)FuturesBulk"

# 3. back_office_preferred_exch_corporate_actions
python -m data_alchemy.main --vendor bloomberg --dataset bbocax_cwiq_pipe --version 1.0 --backfill 200 --ignore-db --file-pattern "(pfdExch(Asia|Euro|Lamr|Namr)CorporateActions|pfd_exch_(asia|euro|lamr|namr)\.cax)"

# 4. back_office_currency
python -m data_alchemy.main --vendor bloomberg --dataset bbocax_cwiq_pipe --version 1.0 --backfill 200 --ignore-db --file-pattern "[Cc]urncy"

# 5. back_office_supplemental
python -m data_alchemy.main --vendor bloomberg --dataset bbocax_cwiq_pipe --version 1.0 --backfill 200 --ignore-db --file-pattern "(condition_code|fields\.|lookup\.)"

# 6. back_office_futures_extended
python -m data_alchemy.main --vendor bloomberg --dataset bbocax_cwiq_pipe --version 1.0 --backfill 200 --ignore-db --file-pattern "(share|nonShare)FuturesExtended"

# 7. back_office_equities
python -m data_alchemy.main --vendor bloomberg --dataset bbocax_cwiq_pipe --version 1.0 --backfill 200 --ignore-db --file-pattern "(equity(Asia1|Asia2|Euro|Lamr|Namr|MifidEuro)(?!.*(CorporateActions|\.cax))|equity_(asia1|asia2|euro|lamr|namr|mifid_euro)(?!\.cax))"

# 8. back_office_preferred_exch
python -m data_alchemy.main --vendor bloomberg --dataset bbocax_cwiq_pipe --version 1.0 --backfill 200 --ignore-db --file-pattern "(pfdExch(Asia|Euro|Lamr|Namr)(?!.*(CorporateActions|\.cax))|pfd_exch_(asia|euro|lamr|namr)(?!\.cax))"
```

---

### Directory Rename

```bash
# mv preserves mtime - no rsync needed
mv /sf/proj/data_alchemy/thanudev/data/bloomberg/1.0 /sf/proj/data_alchemy/thanudev/data/bloomberg/bbocax_cwiq_pipe
```

---

## Notes

- `mv` preserves mtime (same filesystem)
- `--file-pattern` is REGEX not GLOB
- `--backfill` accepts hours only (200 hours ≈ 8 days)
- `--backfill-range` not merged yet (on feature branch)
- Python 3.9 on server - need `pyarrow<22.0.0`
- Package name is `pyyaml` not `yaml`
- Use `.venv/bin/python -m pip` to avoid conda conflict
- Grabber maps: preferred_exch and corporate_actions are mutually exclusive

## Next Steps
- [ ] Complete rsync copy of bloomberg data
- [ ] Run data-alchemy with split service patterns
- [ ] Merge feature/backfill-date-range-clean to dev
- [ ] Merge feature/bbocax-preferred-exch to dev
- [ ] Implement per-dataset backfill hours configuration
