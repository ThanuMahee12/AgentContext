# 2026-01-11

## Project: alchmy

### What I did
- Set up AgentContext repo for global session management
- Configured MkDocs with GitHub Pages CI/CD
- Organized structure: sessions/ + notes/

### Notes
- Moved session management from alchmy repo to global level
- Single markdown file per project in notes/
- Auto-deploy on push via GitHub Actions

---

## Project: alchmydb

### What I did
- Added `alchemy_service` table to track systemd services running data-alchemy pipelines
- Created 20 service records (15 vendor-version-dataset + 5 vendor-only)
- Updated `alchemy_server` with ny5-predpalch01 from ansible inventory
- Created `alchmydb/CLAUDE.md` for Claude Code guidance
- Optimized `alchmy/CLAUDE.md` with build commands

### Schema: alchemy_service
```sql
CREATE TABLE alchemy_service (
    service_id, server_id, raw_id, vendor_id,
    dataset_name, dataset_version, service_name,
    environment, exec_script, watch_interval,
    fp_prefix, raw_fp_prefix, log_path, playbook_file
);
```

### Source references
- Service files: `data-alchemy/build/service-files/`
- Playbook: `deploy-data-alchemy-prod06.yml`
- Inventory: `data-alchemy/build/inventory/hosts.yml`

---

## Project: alchmydb (continued)

### Filetype Normalization

Refactored `raw_filetype` from flat table to normalized structure:

**Before (flat):**
```
raw_filetype: filetype_id, raw_id, extension, mime_type
```

**After (normalized):**
```
filetype (lookup)           raw_filetype (bridge)
├── filetype_id PK          ├── raw_filetype_id PK
├── extension UNIQUE        ├── raw_id FK
├── mime_type               ├── filetype_id FK
├── category                └── is_primary
└── description
```

**Categories:**
- data: csv, parquet, json, xml, txt
- archive: gz, zip, lz4
- spreadsheet: xlsx
- vendor: dfrt, cov, rsk (MSCI)
- control: flg

**Benefit:** Reusable for bronze, gold, cdp layers

---

---

## Project: pathseek

Path analysis tool that scans directories and extracts regex patterns.

**Repo:** https://github.com/ThanuMahee12/pathseek

**Core idea:** Scan folder → Analyze paths → Output patterns

**CLI Flags:**
| Flag | Short | Description |
|------|-------|-------------|
| `base_path` | - | Base path to scan |
| `--full` | `-F` | Full paths list |
| `--pattern` | `-p` | Per-path patterns |
| `--single` | `-s` | Single combined pattern |
| `--unique` | `-u` | Deduplicated patterns |
| `--to-regex` | `-r` | Output as regex |
| `--to-glob` | `-g` | Output as glob |
| `--output` | `-o` | Save to file (txt) |
| `--depth` | `-d` | Max depth |
| `--files-only` | `-f` | Files only |
| `--dirs-only` | `-D` | Dirs only |
| `--count` | `-c` | Count only |

**Output examples:**
```bash
# --full
/sf/data/bloomberg/2025/01/01/a.txt

# --pattern --to-regex
/sf/data/bloomberg/\d{4}/\d{2}/\d{2}/a\.txt

# --pattern --to-glob
/sf/data/bloomberg/*/*/*/*

# --single --to-regex
/sf/data/bloomberg/\d{4}/\d{2}/\d{2}/(a|b|c)\.txt

# Save to file
python pathseek.py /sf/data --pattern --to-regex -o patterns.txt
```

**Structure:**
```
pathseek/
├── util/
│   ├── path_utils.py    # walk_paths(), filter_paths()
│   ├── output_utils.py  # output_list lambda
│   └── cli_utils.py     # add_common_args()
├── pathseek.py          # Main CLI (TODO)
├── pattern_extractor.py # Pattern extraction (TODO)
├── CLAUDE.md
├── CHANGELOG.md
├── LICENSE
├── README.md
└── .gitignore
```

**Key decisions:**
- Generator pattern for memory efficiency
- Lambda for simple functions
- Reusable CLI args in cli_utils.py

**Next:**
- Create `pattern_extractor.py` - Pattern detection logic
- Create `pathseek.py` - Main CLI
- Update README/CLAUDE.md with transformation flags

---

---

## Project: data-alchemy (bbocax mapping)

### Branch
`feature/bbocax-new-mapping`

### Investigation: Missing bbocax back_office mappings

**Current state:**
- Only 4 grabber map types exist for bbocax_cwiq_pipe:
  - backoffice (pfdExch parquet corporate actions)
  - backoffice_cax (pfdExch .cax corporate actions)
  - corporate_actions (equity .cax)
  - corporate_actions_parquet (equity .parquet)

**Missing back_office datasets (from shovel bbo.py):**
| Subproduct | Package | Table Count |
|------------|---------|-------------|
| futures | non_share_futures_extended | 120 |
| equity_options | - | 124 |
| futures | share_futures_extended | 90 |
| futures | share_futures | 56 |
| futures | non_share_futures | 51 |
| equity | - | 40 |
| preferred_exch | - | 24 |
| supplemental | - | 7 |
| currency | - | 5 |

**Key files examined:**
- `tfe-cdp-shovel/shovel/dataset_mapping/bbo.py` - shovel mapping logic
- `tfe-cdp-shovel/shovel/dataset_mapping/bbo_product_mapping.json` - 544 table→subproduct mappings
- `/sf/proj/data_alchemy/assets/cdp_out/bloomberg/back_office_*` - platinum output structure

**Understanding:**
1. Decrypt inclusions control which .enc files get decrypted at silver
2. Grabber maps control gold→platinum path transformation
3. Shovel does simple copy with path restructure (no data transform for most files)
4. Some files stay compressed (.gz), some get decompressed

**Platinum output patterns:**
- `back_office_convert_bonds` → `.dif.gz`, `.out.gz` (keep compressed + date)
- `back_office_futures` → `.dif`, `.out` (decompress only)
- `back_office_preferred_exch_corporate_actions` → `.ndjson`, `.parquet`, `.cax` (transform + raw)

### Next steps
- Create mapping table: source pattern → platinum path
- Update decrypt inclusions for all file types
- Create grabber maps for each back_office subproduct

---

### Next steps (tomorrow)
- CDP Target tables: `cdp_vendor`, `cdp_dataset`, `cwiq_cdp_mapping`
